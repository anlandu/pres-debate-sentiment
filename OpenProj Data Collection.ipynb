{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Presidential Debate Text Processing\n",
    "### Anlan Du\n",
    "11-10-17\n",
    "Open Project  \n",
    "Here, I'll walk through the steps of processing the presidential debates! I break down data into three main sets: speech data, which calculates information speech-by-speech (a speech is dileneated by a new person talking); speaker data, which aggregates all speech data for a given speaker (either Clinton, Trump, or a Moderator); and dictionaries, which provide the most frequently used words per speaker.  \n",
    "[Source of Debate 1 Transcript](https://www.washingtonpost.com/news/the-fix/wp/2016/09/26/the-first-trump-clinton-presidential-debate-transcript-annotated/)  \n",
    "[Source of Debate 2 Transcript](https://medium.com/@PolitiFact/politifacts-annotated-transcript-of-the-second-presidential-debate-b54f45edeb99#.6sodpyhkw)  \n",
    "[Source of Debate 3 Transcript](https://www.washingtonpost.com/news/the-fix/wp/2016/10/19/the-final-trump-clinton-debate-transcript-annotated/)  \n",
    "(I intentionally selected transcripts with structurally similar motifs--names in all caps, interruptions indicated by ellipses--so that I could reduce my analysis to one function, to be repeated thrice.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\twitter\\__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "#Standard import statements\n",
    "import json\n",
    "import string\n",
    "import re\n",
    "import csv\n",
    "from nltk import RegexpTokenizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below function processes each speech and returns data on each one (see speechCols for the specifics of the data). It's unfortunate I had to create such a massive cell, but necessary in order to call it as a function (so I can call it for each debate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_speeches(i):\n",
    "    with open(\"debate%s.txt\"% i, \"r\") as f:\n",
    "        debate = f.read()\n",
    "        \n",
    "    #regex to delineate speeches\n",
    "    re_tokenizer = RegexpTokenizer(\"([A-Z][A-Z]+ ?)+:\")\n",
    "    \n",
    "    #get list of speaker name indices\n",
    "    speechNameIndices = list(zip(re_tokenizer.tokenize(debate), re_tokenizer.span_tokenize(debate)))\n",
    "    speechTextIndices = []\n",
    "    speechData=[[0,0,0,0,0,0,0,0,0,0,0] for i in range(len(speechNameIndices))]\n",
    "\n",
    "    #get the indices of each speech\n",
    "    for i, speech in enumerate(speechNameIndices):\n",
    "        # for purposes of determining speech's x positioning in the visualization,\n",
    "        # append start and end indices to speech info\n",
    "        speechData[i][2]=speechNameIndices[i][1][0]\n",
    "        try:\n",
    "            speechData[i][3]=speechNameIndices[i+1][1][0]-1\n",
    "        except:\n",
    "            speechData[i][3]=len(debate)\n",
    "\n",
    "        # append indices per speech to speechTextIndices\n",
    "        c = [speech[0], [speech[1][1], 0]]\n",
    "        try: \n",
    "            #append ending index\n",
    "            c[1][1] = speechNameIndices[i+1][1][0]\n",
    "        except:\n",
    "            #final section\n",
    "            pass\n",
    "        speechTextIndices.append(c)\n",
    "\n",
    "        \n",
    "    #create array of text of each speech. \n",
    "    for i, speech in enumerate(speechTextIndices):\n",
    "        speech_text=debate[speech[1][0]:speech[1][1]-1]\n",
    "        speechData[i][0]=speech[0]\n",
    "        #annoying special chars we want to get rid of.\n",
    "        speechData[i][1]=speech_text.replace(\"\\n\", \" \").replace(\"\\r\", \"\").replace(\"’\",\"'\").replace(\"“\",\"\\\"\").replace(\"”\",\"\\\"\").strip()\n",
    "        #all-caps words in parentheses are superfluous notes (e.g. \"APPLAUSE\")\n",
    "        speechData[i][1]=re.sub(\"\\([A-Z][A-Z]+\\)\", \"\", speechData[i][1])\n",
    "        \n",
    "    #punctuation (to get rid of)\n",
    "    puncFinal=string.punctuation+\")’(,�...``''--“”\"\n",
    "    #this will eventually end up looking like words[speech][word]\n",
    "    words=[]\n",
    "\n",
    "    #to keep track of sentence number as we iterate through each chapter\n",
    "    currSent=0\n",
    "\n",
    "    for i, speech in enumerate(speechData):\n",
    "        #add sentiment score to speech data\n",
    "        sentAnalyzer=SentimentIntensityAnalyzer()\n",
    "        speech[10]=sentAnalyzer.polarity_scores(speech[1]).get(\"compound\")\n",
    "        speech_text=speech[1]\n",
    "        \n",
    "        #append to chapter a list of this speech's words\n",
    "        words.append(nltk.word_tokenize(speech_text))\n",
    "\n",
    "        #temp contains words to be deleted; this circumvents bugs in removing elements\n",
    "        temp=[]\n",
    "        \n",
    "        #ellipses indicate interruption; increment \"interrupt\" and \"number of sentences\" data as such.\n",
    "        if words[i][-1:]==['...']:\n",
    "            speechData[i][4]+=1\n",
    "            speechData[i][7]+=1\n",
    "\n",
    "        for j,word in enumerate(words[i]):\n",
    "            #set punctuation to be deleted and iterate num sentences appropriately\n",
    "            if word in puncFinal:\n",
    "                if word=='.' or word=='?' or word=='!':\n",
    "                    speechData[i][7]+=1\n",
    "                temp.append(word)\n",
    "                \n",
    "            #add non-punctuation words to data (including word length)\n",
    "            else:\n",
    "                speechData[i][5]+=1\n",
    "                speechData[i][6]+=len(word)\n",
    "\n",
    "            #at end of each sentence, delete unwanted punctuation\n",
    "            if j==len(words[i])-1:\n",
    "                while len(temp) != 0:\n",
    "                    words[i].remove(temp[0])\n",
    "                    temp.remove(temp[0])\n",
    "                    \n",
    "        #average word length = num chars/num words\n",
    "        speechData[i][8]=speechData[i][6]/speechData[i][5]\n",
    "        speechData[i][9]=speechData[i][5]/speechData[i][7]\n",
    "        \n",
    "        from collections import Counter\n",
    "        from nltk.corpus import stopwords\n",
    "\n",
    "        #the word no is important. the word said isn't. modifying stopwords as such\n",
    "        stopset=set(stopwords.words('english'))\n",
    "        stopset.remove('no')\n",
    "        stopset.add(\"'s\")\n",
    "        stopset.add(\"'ve\")\n",
    "        stopset.add(\"'re\")\n",
    "        stopset.add(\"n't\")\n",
    "        stopset.add(\"'m\")\n",
    "        fullDict=[]\n",
    "\n",
    "        trumpDict=Counter()\n",
    "        clintonDict=Counter()\n",
    "        modDict=Counter()\n",
    "        #add words to each speaker's dictionary, then append the (sorted) 75 most common words for each\n",
    "        #speaker to the nested array \"fullDict\"\n",
    "        for i, speech in enumerate(words):\n",
    "            toBeAdded=[word.lower() for word in words[i] if word.lower() not in stopset]\n",
    "            if speechData[i][0]=='TRUMP':\n",
    "                trumpDict.update(toBeAdded)\n",
    "            elif speechData[i][0]=='CLINTON':\n",
    "                clintonDict.update(toBeAdded)\n",
    "            else:\n",
    "                modDict.update(toBeAdded)\n",
    "        fullDict.append(trumpDict.most_common(60))\n",
    "        fullDict.append(clintonDict.most_common(60))\n",
    "        fullDict.append(modDict.most_common(60))\n",
    "    \n",
    "    #because we can only return one object, return array of dicts and speech data\n",
    "    return [fullDict,speechData]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solid! So we've got our speech data worked out; now we just need to work out our data for each speaker. We'll go back through our already-collected data and do calculations with those numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_speakers(speechData):\n",
    "    speakerData=[[0 for i in range(7)] for j in range(3)]\n",
    "    #append names to speaker data\n",
    "    speakerData[0][0]=\"Trump\"\n",
    "    speakerData[1][0]=\"Clinton\"\n",
    "    speakerData[2][0]=\"Moderator\"\n",
    "    #create arrays to tally words, chars, etc.; we don't need them in final dataset\n",
    "    #so these are just throwaway variables\n",
    "    totalWords=[0,0,0]\n",
    "    totalChars=[0,0,0]\n",
    "    totalSents=[0,0,0]\n",
    "    totalSentiment=[0,0,0]\n",
    "    totalInterrupt=[0,0,0]\n",
    "    #tally data from each speech in appropriate speaker's arrays\n",
    "    for i, speech in enumerate(speechData):\n",
    "        sInd=speakInd(speech[0])\n",
    "        speakerData[sInd][1]+=1\n",
    "        totalWords[sInd]+=speech[5]\n",
    "        totalChars[sInd]+=speech[6]\n",
    "        totalSents[sInd]+=speech[7]\n",
    "        speakerData[sInd][2]+=speech[4]\n",
    "        totalSentiment[sInd]+=speech[10]\n",
    "    for i in range(3):\n",
    "        #calc words per speech\n",
    "        speakerData[i][3]=totalWords[i]/speakerData[i][1]\n",
    "        #calc avg word length per speaker\n",
    "        speakerData[i][4]=totalChars[i]/totalWords[i]\n",
    "        #calc avg sentence length per speaker\n",
    "        speakerData[i][5]=totalWords[i]/totalSents[i]\n",
    "        #calc avg sentiment\n",
    "        speakerData[i][6]=totalSentiment[i]/speakerData[i][1]\n",
    "    return speakerData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#helper function; returns index of speaker in speaker data array based on speaker name\n",
    "#(0 is trump, 1 is clinton, 2 is moderator)\n",
    "def speakInd(speaker):\n",
    "    if speaker=='TRUMP':\n",
    "        return 0\n",
    "    elif speaker=='CLINTON':\n",
    "        return 1\n",
    "    else:\n",
    "        return 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we just need to save everything and we're done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "speakerDataCols=[\"speaker\",\"numSpeeches\",\"interrupted\",\"avgWordsPerSpeech\",\"avgWordLen\",\"avgSentLen\",\"sentiment\"]\n",
    "speechCols=[\"speaker\",\"speechText\",\"startInd\",\"endInd\",\"interrupted\",\"numWords\",\"numChars\",\"numSents\",\"avgWordLen\",\"avgSentLen\",\"sentiment\"]\n",
    "dictCols=[\"word\",\"freq\"]\n",
    "for i in range(1,4):\n",
    "    #data[0] is an array of dicts per speaker; data[1] is data per speech\n",
    "    data=process_speeches(i)\n",
    "    #use speech data to get speaker data\n",
    "    speakData=process_speakers(data[1])\n",
    "    \n",
    "    #write speech data\n",
    "    with open('debate%s.csv'% i, 'w',newline='') as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        writer.writerow(speechCols)\n",
    "        for row in data[1]:\n",
    "            writer.writerow(row)\n",
    "            \n",
    "    #write speaker dicts\n",
    "    with open('trumpDict%s.csv'% i, 'w',newline='') as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        writer.writerow(dictCols)\n",
    "        for key,value in data[0][0]:\n",
    "            writer.writerow([key, value])\n",
    "    with open('clintonDict%s.csv'% i, 'w',newline='') as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        writer.writerow(dictCols)\n",
    "        for key,value in data[0][1]:\n",
    "            writer.writerow([key, value])\n",
    "    with open('moderatorDict%s.csv'% i, 'w',newline='') as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        writer.writerow(dictCols)\n",
    "        for key,value in data[0][2]:\n",
    "            writer.writerow([key, value])\n",
    "            \n",
    "    #write speaker data\n",
    "    with open('speakData%s.csv'% i, 'w',newline='') as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        writer.writerow(speakerDataCols)\n",
    "        for row in speakData:\n",
    "            writer.writerow(row)\n",
    "# grab all lines that are all caps\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
